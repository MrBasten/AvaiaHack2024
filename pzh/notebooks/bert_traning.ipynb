{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BankiDataset(Dataset):\n",
    "    def __init__(self, tokenizer: AutoTokenizer, dataset_path: str = \"../data/processed/promsvyazbank_reviews.csv\"):\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        tokenized_\n",
    "        for idx, row in tqdm(df, total=len(df)):\n",
    "            \n",
    "            tokenizer()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index]\n",
    "        input_text = \"Название: {}\\nОценка:{}\\n\\n{}\".format(row[\"title\"], row[\"grattitue\"], row[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"ai-forever/ru-en-RoSBERTa\",\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-5,\n",
    "    \"eval_batch_size\": 4,\n",
    "    \"epochs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(row):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REVIEW_CLS = 3\n",
    "NUM_FINANCIAL_CLS = 2\n",
    "NUM_REASON_CLS = 4\n",
    "TOTAL_CLS = NUM_REVIEW_CLS + NUM_FINANCIAL_CLS + NUM_REASON_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(row):\n",
    "    label = [0]*TOTAL_CLS\n",
    "    label[row.review_category] = 1\n",
    "    label[row.financial + NUM_REVIEW_CLS] = 1\n",
    "    label[row.reason_category + NUM_FINANCIAL_CLS + NUM_REVIEW_CLS] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( \"../data/processed/promsvyazbank_reviews.csv\")\n",
    "df = df.set_index(\"id\")\n",
    "df[\"bert_text\"] = df.apply(lambda row: \"classification: Название: {}\\nОценка: {}\\n\\n{}\".format(row[\"title\"], row[\"grade\"], row[\"text\"]), axis=1)\n",
    "df[\"label\"] = df.apply(label_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"bert_text\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bert_text', 'label', 'id'],\n",
       "        num_rows: 7093\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['bert_text', 'label', 'id'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f01ce3e5a784ed584f55f3834043e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model'])\n",
    "dataset = dataset.map(lambda examples: tokenizer(examples[\"bert_text\"], truncation=True, padding=\"max_length\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "REVIEW_CLS_IND = range(0, 3)\n",
    "FINANCIAL_CLS_IND = range(3, 4)\n",
    "REASON_CLS_IND = range(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_from_logits(logits):\n",
    "    ret = np.zeros(logits.shape)\n",
    "    \n",
    "    # The first 5 columns (GLOBAL_SCORE_INDICES) are for global scores. They should be handled with a multiclass approach\n",
    "    # i.e. we fill 1 to the class with highest probability, and 0 into the other columns\n",
    "    best_class = np.argmax(logits[:, REVIEW_CLS_IND], axis=-1)\n",
    "    ret[list(range(len(ret))), best_class] = 1\n",
    "    \n",
    "    # The other columns are for causes and emotions. They should be handled with multilabel approach.\n",
    "    # i.e. we fill 1 to every class whose score is higher than some threshold\n",
    "    # In this example, we choose that threshold = 0\n",
    "    ret[:, FINANCIAL_CLS_IND] = (logits[:, FINANCIAL_CLS_IND] >= 0).astype(int)\n",
    "    ret[:, REASON_CLS_IND] = (logits[:, REASON_CLS_IND] >= 0).astype(int)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "    \n",
    "    # Deduce predictions from logits\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "    \n",
    "    # Get f1 metrics for global scoring. Notice that f1_micro = accuracy\n",
    "    final_metrics[\"f1_micro_for_review_score\"] = f1_score(labels[:, REVIEW_CLS_IND], predictions[:, REVIEW_CLS_IND], average=\"micro\")\n",
    "    final_metrics[\"f1_macro_for_review_score\"] = f1_score(labels[:, REVIEW_CLS_IND], predictions[:, REVIEW_CLS_IND], average=\"macro\")\n",
    "    \n",
    "    # Get f1 metrics for causes\n",
    "    final_metrics[\"f1_micro_for_financial\"] = f1_score(labels[:, FINANCIAL_CLS_IND], predictions[:, FINANCIAL_CLS_IND], average=\"micro\")\n",
    "    final_metrics[\"f1_macro_for_financial\"] = f1_score(labels[:, FINANCIAL_CLS_IND], predictions[:, FINANCIAL_CLS_IND], average=\"macro\")\n",
    "    \n",
    "    # Get f1 metrics for emotions\n",
    "    final_metrics[\"f1_micro_for_reason\"] = f1_score(labels[:, REASON_CLS_IND], predictions[:, REASON_CLS_IND], average=\"micro\")\n",
    "    final_metrics[\"f1_macro_for_reason\"] = f1_score(labels[:, REASON_CLS_IND], predictions[:, REASON_CLS_IND], average=\"macro\")\n",
    "\n",
    "    # The global f1_metrics\n",
    "    final_metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n",
    "    final_metrics[\"f1_macro\"] = f1_score(labels, predictions, average=\"macro\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification report for global scores: \")\n",
    "    print(classification_report(labels[:, REVIEW_CLS_IND], predictions[:, REVIEW_CLS_IND], zero_division=0))\n",
    "    print(\"Classification report for causes: \")\n",
    "    print(classification_report(labels[:, FINANCIAL_CLS_IND], predictions[:, FINANCIAL_CLS_IND], zero_division=0))\n",
    "    print(\"Classification report for emotions: \")\n",
    "    print(classification_report(labels[:, REASON_CLS_IND], predictions[:, REASON_CLS_IND], zero_division=0))\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "\n",
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.group_weights = group_weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        global_score_loss = torch.nn.functional.cross_entropy(logits[:, REVIEW_CLS_IND], labels[:, REVIEW_CLS_IND])\n",
    "        emotion_loss = torch.nn.functional.binary_cross_entropy_with_logits(logits[:, FINANCIAL_CLS_IND], labels[:, FINANCIAL_CLS_IND])\n",
    "        cause_loss = torch.nn.functional.binary_cross_entropy_with_logits(logits[:, REASON_CLS_IND], labels[:, REASON_CLS_IND])\n",
    "        \n",
    "        loss = self.group_weights[0] * global_score_loss + self.group_weights[2] * emotion_loss + self.group_weights[1] * cause_loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification(config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/psh\",\n",
    "    learning_rate=config['lr'],\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    per_device_eval_batch_size=config['eval_batch_size'],\n",
    "    num_train_epochs=config['epochs'],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrinterCallback],\n",
    "    group_weights=(0.7, 4, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
